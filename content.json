{"pages":[{"title":"About 1인분 하고픈 DS","text":"늦은 나이에 데이터 분석이라는 직군을 알게되어 별 볼일 없는 모든 커리어를 접어둔채 데이터 탐색을 시작했습니다. 블로그를 통해 여러 사람들과 소통하고 싶고 여러 사람들에게 도움을 주고 싶습니다. 꾸준히 10년 이상 블로그 활동하려 합니다. 사람답게 1인분을 하고자 노력하고 있습니다. 혹시나 필요하실 수도 있어서 남깁니다. MacBook Pro (16-inch, 2019) 프로세서 : 2.3 GHz 8코어 Intel Core i9 메모리 : 16GB 2667 MHz DDR4 그래픽 : Intel UHD Graphics 630 1536 MB 블로그에 실린 모든 내용은 출처가 있는 내용을 저의 지극히 주관적인 관점으로 재해석해 올린 내용들입니다. 게시글 내용에 수정이 필요하거나 게시글이 불편하여 내리길 원하시면 아래 e-mail로 보내주시면 검토하여 진행하도록 하겠습니다. 감사합니다. About 페이지를 만드는데 도움주신 분께 감사드립니다. 출처 : https://kinetic27.github.io/2020/03/06/build-blog-with-hexo-github/#About-%ED%8E%98%EC%9D%B4%EC%A7%80","link":"/about/index.html"},{"title":"all-tags","text":"","link":"/all-tags/index.html"},{"title":"all-archives","text":"","link":"/all-archives/index.html"},{"title":"all-categories","text":"","link":"/all-categories/index.html"}],"posts":[{"title":"Hexo 블로그 만들기","text":"안녕하세요!hexo 블로그 만드는 방법입니다. hexo 블로그 생성전 아래 2가지를 먼저 설치해야 합니다. Node.js Should be at least Node.js 8.10, recommends 10.0 or higher Git github에서 repository를 생성해야 합니다. 참고 사이트 hexo 블로그 공식 홈페이지 hexo 블로그 설치하기 공식문서 터미널을 엽니다. 블로그를 만들면 폴더가 생성됩니다. 폴더를 생성하고자 하는 곳으로 가셔서 아래의 $를 제외한 명령어를 입력하세요. 띄어쓰기 대소문자 모두 그대로 입력해주세요. 121. $ npm install hexo-cli -g2. $ hexo init blog 블로그를 만든 폴더로 이동합니다. 1235. $ cd blog6. $ npm install7. $ hexo server 로컬에서 hexo 블로그를 확인하실 수 있습니다. 아래 터미널의 경로로 가셔서 파일을 만듭니다. 터미널에서의 경로 본인이 만든 폴더 안의 source 폴더 안에 _post 1$ source/_posts/ ** 파일 이름(본인이 원하는 이름을 적으시면 됩니다.)** 파일명.md 파일 안에 원하는 내용을 적는데, 맨 위에 제목은 아래처럼 작성해주세요. 12345678---title: Hexo에 글을 쓰고 사이트에 반영하기---위처럼 제목을 적으셨으면 아래줄에 내용을 입력하면 됩니다. 1번 내용2번 내용 등주석을 적을 때는 `#`을 사용하여 적으시면 됩니다. 마크다운 문법 올릴 내용이 준비되었다면 아래와 같이 4가지 단계를 거쳐 블로그에 내용을 올리시면 됩니다. 1번: 헥소에 변화를 주겠다고 얘기한다.2번: 헥소에 내용을 반영하겠다고 얘기한다.3번: 헥소에 반영된 내용을 확인하겠다고 얘기한다. 수정이 필요하면 수정을 진행한다. 수정이 완료되면 아래 4번을 통해 내용을 공개하겠다고 얘기한다.4번: 블로그에 내용을 게시하겠다고 얘기한다. 터미널 창으로 가셔서 $ 표시 뒤의 글자를 치세요.1234$ hexo clean$ hexo generate$ hexo server # 생략가능, 로컬영역에서 확인$ hexo deploy 추가적인 내용은 지속적으로 편집하도록 하겠습니다. 최대한 자세히 쓰려고 노력중입니다.여러가지 의견 감사히 받겠습니다.댓글을 어떻게 활성화해야 하는지 몰라 여기저기 구글링중입니다. 댓글기능을 추가하였습니다.","link":"/2020/04/05/Hexo%20create/"},{"title":"favicon","text":"favicon 출처","link":"/2020/04/25/favicon/"},{"title":"Agile","text":"Agile(애자일) 방법론 3줄 요약 작업 계획을 짧은 단위로 세우고 시제품을 만들어 나가는 사이클을 반복 고객의 요구 변화에 유연하고도 신속하게 대응하는 개발 방법론 자료출처 요즘 에자일, 에자일 기법에 대해 2번 이상 듣기도 하고 기업에서도 에자일 기법으로 프로세스를 진행한다고 하니 무엇인지에 대한 개념도 알아보고 내 삶에 적용 시키기도 해볼 겸해서 간단하게 기록하려합니다. 우선, 애자일 방법이란 ?기업경영 및 소프트웨어 등의 개발을 고객중심으로 진행하는 방법론 애자일방식의 조직운영 고객중심 아웃풋 중심 유연하고도 민첩한 대응력 자율성과 권한을 가진 조직 운영 에자일 개발 선언문 애자일 방법은 급변화하고 진화하고 있는 환경에 효과적으로 대응할 수 있는 방법이라고 생각됩니다. 피드백 댓글로 남겨주세요~!감사합니다 :)","link":"/2020/04/08/Agile/"},{"title":"visited_count","text":"블로그 방문자 수 설정하기 참조 사이트","link":"/2020/04/17/Hexo-visited-count/"},{"title":"Hexo profile 변경하기","text":"안녕하세요, Hexo Icarus theme의 프로필 사진을 변경하는 방법에 대해 알아보겠습니다. 아래와 같이 터미널에서 source 폴더 안에 img 폴더를 만든 후 img 폴더 안에 이미지 사진을 올리고 아래와 같이 작성하면 됩니다. 파일이름 대소문자를 구분한다고 하니 동일하게 진행하셔야 이미지가 보입니다. ![문자 사이에 공간이 없어야 합니다] 즉, 대쉬를 이용해서 문자와 문자 사이에 공간을 없애야 정상적으로 작동합니다. upload까지 약 3~4분 정도 소요되니 참고하시길 바랍니다. 1$ mkdir img Hexo 프로필 사진 변경 Hexo 프로필 아래 twitter 아이콘 제거하기 Hexo 블로그에 이미지 올리기 출처: 이카루스 테마 페이지","link":"/2020/04/25/Hexo-profile/"},{"title":"안녕하세요 !!","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/04/02/hello-world/"},{"title":"markdown 문법에 대하여","text":"markdown 문법 사용법 출처 python is really cool python pandas depth1 depth2 numpy java c++ golang python java c++ 파이썬은 재밌다. y = 3x $y = 3x$ 12a = 10print(a) 12a = 10print(a) 파이썬 공식 페이지","link":"/2020/04/05/markdown-0/"},{"title":"porject EDA","text":"EDA project 처음으로 진행했던 프로젝트 아파트 가격분석 EDA 자세한 내용은 아래 깃헙을 참고해주세요. EDA project Analysis APT price","link":"/2020/04/23/porject-EDA/"},{"title":"project-LinearRegression","text":"Linear Regression project 한달동안 팀원들과 함께 한 프로젝트 홍성현님, 유호원님, 배준영님 한달동안 수고 많았습니다 :) 자료는 아래 깃헙에서 확인해주세요. Linear Regression project UsedCar price","link":"/2020/04/25/project-LinearRegression/"},{"title":"test.html","text":"BeautifulSoup test Contents Title Test contents 추후 작성할 포스트 1. python os 2.","link":"/2020/04/21/test-html/"},{"title":"webcrawling","text":"1. 개발자 도구를 활용한 웹페이지 분석 chrome 기준 웹 브라우저 : html로 작성된 내용을 user(사람들)가 보기 쉽게 랜더링 해주는 기능을 함 Elements 탭 원하는 데이터로 이동하는 기능을 사용 엘레먼트 탭 : 왼쪽 상단에 있음, 원하는 데이터를 클릭했을 때 어떤 테그와 어떤 속성을 가지는지 표시해주는 역할을 함 어떤 태그와 속성을 가지는지 먼저 파악해야 함 Network 탭 Preserve log : 체크 시, 로그가 지워지지 않고 유지됨 브라우저가 서버에 요청되는 모든 요청을 로그함 url 확인 가능 요청이 많은 이유 : 이미지 등은 첫번째 요청에 한번에 오지 않고, 따로 요청하여 받아옴 처음엔 기본적인 데이터만 넘기고, 나머지 데이터는 브라우저에서 ajax 등의 기술을 이용해서 비동기적으로 가져갈 수 있도록 함 HTTP(Hyper Text Transfer Protocol) : HTML 문서 등의 리소스를 전송하는 프로토콜(규약) 클라이언트(user가 사용하는 브라우저)가 서버에 HTTP 요청(Get, Post 등) 서버에서 클라이언트로 HTTP 응답을 함 Get 요청 : 데이터를 url에 포함하여 전달(주로 리소스 요청에 사용), 정보의 공유가능 Post 요청 : 데이터를 Form data에 포함하여 전달(주로 로그인에 사용) rendering(렌더링) : html을 받아 사용자(사람들)이 볼 수 있도록 출력해주는 작업","link":"/2020/04/03/webcrawling-1/"},{"title":"webcrawling","text":"※ 출처 : fast campus 머신러닝 인강(변영효 강사님) 일부 내용만 발췌하였고, 기본적인 개념 및 추가내용을 확인하시려면 인강 수강을 권장드립니다. 내용요약 웹사이트에서 원하는 정보의 태그를 파악 모듈을 통해 태그를 찾은 후 원하는 값을 가져옴 2. HTML(Hyper Text Markup Language) 웹 사이트를 생성하기 위한 언어로 문서와 문서가 링크로 연결되어 있고, 태그를 사용하는 언어 태그 : HTML 문서의 기본 블락 브라우저에 어떻게 렌더링(화면에 표시)될지 전달 &lt;태그명 속성1=”속성값1” 속성2=”속성값2”&gt;Value&lt;/태그명&gt; &lt;태그명 속성1=”속성값1” 속성2=”속성값2”/&gt; p 태그 : paragraph tag 한 문단으로 표시해주는 태그 div 태그 그룹핑을 하는 태그 대부분의 crawling은 태그 안에 있는 값을 추출하는 작업입니다. html 기본구조 BeautifulSoup test Contents Title Test contents Test Test Test 1 Test Test Test 2 Test Test Test 3 웹 사이트에서 본인에게 필요한 정보를 가져오는 실습을 해보는걸 추천드립니다.","link":"/2020/04/21/webcrawling-2/"},{"title":"re(regular expression)","text":"정규표현식 regular expression 특정한 패턴과 일치하는 문자열를 ‘검색’, ‘치환’, ‘제거’ 하는 기능을 지원 정규표현식의 도움없이 패턴을 찾는 작업(Rule 기반)은 불완전 하거나, 작업의 cost가 높음 e.g) 이메일 형식 판별, 전화번호 형식 판별, 숫자로만 이루어진 문자열 등 raw string 문자열 앞에 r이 붙으면 해당 문자열이 구성된 그대로 문자열로 변환 12345a = 'abcdef\\n' # escapce 문자열print(a)b = r'abcdef\\n'print(b) 기본 패턴 a, X, 9 등등 문자 하나하나의 character들은 정확히 해당 문자와 일치 e.g) 패턴 test는 test 문자열과 일치 대소문자의 경우 기본적으로 구별하나, 구별하지 않도록 설정 가능 몇몇 문자들에 대해서는 예외가 존재하는데, 이들은 틀별한 의미로 사용 됨 . ^ $ * + ? { } [ ] \\ | ( ) . (마침표) - 어떤 한개의 character와 일치 (newline(엔터) 제외) \\w - 문자 character와 일치 [a-zA-Z0-9_] \\s - 공백문자와 일치 \\t, \\n, \\r - tab, newline, return \\d - 숫자 character와 일치 [0-9] ^ = 시작, $ = 끝 각각 문자열의 시작과 끝을 의미 \\가 붙으면 스페셜한 의미가 없어짐. 예를들어 \\.는 .자체를 의미 \\\\는 \\를 의미 자세한 내용은 링크 참조 https://docs.python.org/3/library/re.html search method 첫번째로 패턴을 찾으면 match 객체를 반환 패턴을 찾지 못하면 None 반환 12345678910import rem = re.search(r'abc', '123abdef')mm = re.search(r'\\d\\d\\d\\w', '112abcdef119')mm = re.search(r'..\\w\\w', '@#$%ABCDabcd')m metacharacters (메타 캐릭터)[] 문자들의 범위를 나타내기 위해 사용 [] 내부의 메타 캐릭터는 캐릭터 자체를 나타냄 e.g) [abck] : a or b or c or k [abc.^] : a or b or c or . or ^ [a-d] : -와 함께 사용되면 해당 문자 사이의 범위에 속하는 문자 중 하나 [0-9] : 모든 숫자 [a-z] : 모든 소문자 [A-Z] : 모든 대문자 [a-zA-Z0-9] : 모든 알파벳 문자 및 숫자 [^0-9] : ^가 맨 앞에 사용 되는 경우 해당 문자 패턴이 아닌 것과 매칭 1234567re.search(r'[cbm]at', 'aat')re.search(r'[0-4]haha', '7hahah')re.search(r'[abc.^]aron', 'daron')re.search(r'[^abc]aron', '0aron') \\ 다른 문자와 함께 사용되어 특수한 의미를 지님 \\d : 숫자를 [0-9]와 동일 \\D : 숫자가 아닌 문자 [^0-9]와 동일 \\s : 공백 문자(띄어쓰기, 탭, 엔터 등) \\S : 공백이 아닌 문자 \\w : 알파벳대소문자, 숫자 [0-9a-zA-Z]와 동일 \\W : non alpha-numeric 문자 [^0-9a-zA-Z]와 동일 메타 캐릭터가 캐릭터 자체를 표현하도록 할 경우 사용 \\. , \\\\ 123re.search(r'\\Sand', 'apple land banana')re.search(r'\\.and', '.and') . 모든 문자를 의미 1re.search(r'p.g', 'pig') 반복패턴 패턴 뒤에 위치하는 *, +, ?는 해당 패턴이 반복적으로 존재하는지 검사 ‘+’ -&gt; 1번 이상의 패턴이 발생 ‘*’ -&gt; 0번 이상의 패턴이 발생 ‘?’ -&gt; 0 혹은 1번의 패턴이 발생 반복을 패턴의 경우 greedy하게 검색 함, 즉 가능한 많은 부분이 매칭되도록 함 e.g) a[bcd]*b 패턴을 abcbdccb에서 검색하는 경우 ab, abcb, abcbdccb 전부 가능 하지만 최대한 많은 부분이 매칭된 abcbdccb가 검색된 패턴 1234567891011re.search(r'a[bcd]*b', 'abcbdccb')re.search(r'b\\w+a', 'banana')re.search(r'i+', 'piigiii')re.search(r'pi+g', 'pg')re.search(r'pi*g', 'pg')re.search(r'https?', 'http://www.naver.com') ^, $ ^ 문자열의 맨 앞부터 일치하는 경우 검색 $ 문자열의 맨 뒤부터 일치하는 경우 검색 123456789re.search(r'b\\w+a', 'cabana')re.search(r'^b\\w+a', 'cabana')re.search(r'^b\\w+a', 'babana')re.search(r'b\\w+a$', 'cabana')re.search(r'b\\w+a$', 'cabanap') grouping ()을 사용하여 그루핑 매칭 결과를 각 그룹별로 분리 가능 패턴 명시 할 때, 각 그룹을 괄호() 안에 넣어 분리하여 사용 1234m = re.search(r'(\\w+)@(.+)', 'test@gmail.com')print(m.group(1))print(m.group(2))print(m.group(0)) {} *, +, ?을 사용하여 반복적인 패턴을 찾는 것이 가능하나, 반복의 횟수 제한은 불가 패턴뒤에 위치하는 중괄호{}에 숫자를 명시하면 해당 숫자 만큼의 반복인 경우에만 매칭 {4} - 4번 반복 {3,4} - 3 ~ 4번 반복 1re.search('pi{3,5}g', 'piiiiig') 미니멈 매칭(non-greedy way) 기본적으로 *, +, ?를 사용하면 greedy(맥시멈 매칭)하게 동작함 *?, +?을 이용하여 해당 기능을 구현 123re.search(r'&lt;.+&gt;', '&lt;html&gt;haha&lt;/html&gt;')re.search(r'&lt;.+?&gt;', '&lt;html&gt;haha&lt;/html&gt;') {}? {m,n}의 경우 m번 에서 n번 반복하나 greedy하게 동작 {m,n}?로 사용하면 non-greedy하게 동작. 즉, 최소 m번만 매칭하면 만족 123re.search(r'a{3,5}', 'aaaaa')re.search(r'a{3,5}?', 'aaaaa') match search와 유사하나, 주어진 문자열의 시작부터 비교하여 패턴이 있는지 확인 시작부터 해당 패턴이 존재하지 않다면 None 반환 12345re.match(r'\\d\\d\\d', 'my number is 123')re.match(r'\\d\\d\\d', '123 is my number')re.search(r'^\\d\\d\\d', '123 is my number') findall search가 최초로 매칭되는 패턴만 반환한다면, findall은 매칭되는 전체의 패턴을 반환 매칭되는 모든 결과를 리스트 형태로 반환 1re.findall(r'[\\w-]+@[\\w.]+', 'test@gmail.com haha test2@gmail.com nice test test') sub 주어진 문자열에서 일치하는 모든 패턴을 replace 그 결과를 문자열로 다시 반환함 두번째 인자는 특정 문자열이 될 수도 있고, 함수가 될 수 도 있음 count가 0인 경우는 전체를, 1이상이면 해당 숫자만큼 치환 됨 1re.sub(r'[\\w-]+@[\\w.]+', 'great', 'test@gmail.com haha test2@gmail.com nice test test', count=1) compile 동일한 정규표현식을 매번 다시 쓰기 번거로움을 해결 compile로 해당표현식을 re.RegexObject 객체로 저장하여 사용가능 123email_reg = re.compile(r'[\\w-]+@[\\w.]+')email_reg.search('test@gmail.com haha good')email_reg.findall() 연습문제 아래 뉴스에서 이메일 주소를 추출해 보세요 다음중 올바른 (http, https) 웹페이지만 찾으시오 1234567891011121314151617181920212223242526272829303132333435import requestsfrom bs4 import BeautifulSoup&gt; 위의 두 모듈이 없는 경우에는 pip install requests bs4 실행def get_news_content(url): response = requests.get(url) content = response.text soup = BeautifulSoup(content, 'html5lib') div = soup.find('div', attrs = {'id' : 'harmonyContainer'}) content = '' for paragraph in div.find_all('p'): content += paragraph.get_text() return contentnews1 = get_news_content('https://news.v.daum.net/v/20190617073049838')print(news1)email_reg = re.compile(r'[\\w-]+@[\\w.]+\\w+')email_reg.search(news1)webs = ['http://www.test.co.kr', 'https://www.test1.com', 'http://www.test.com', 'ftp://www.test.com', 'http:://www.test.com', 'htp://www.test.com', 'http://www.google.com', 'https://www.homepage.com.']web_reg = re.compile(r'https?://[\\w.]+\\w+$')list(map(lambda w:web_reg.search(w) != None, webs))","link":"/2020/04/24/re/"},{"title":"webcrawling","text":"python의 request 모듈을 사용하여 http request/resopnse 확인하기requests 모듈 http request/response를 위한 모듈 HTTP method를 메소드 명으로 사용하여 request 요청 예) get, post 123456789101112131415import requests# get 방식url = 'https://news.v.daum.net/v/20190728165812603'resp = requests.get(url)resp.text# post 방식url = 'https://www.kangcom.com/member/member_check.asp'data = { 'id': 'testid', 'pwd': 'password'}resp = requests.post(url, data=data)resp.text HTTP header 데이터 이용하기 header 데이터 구성하기 header 데이터 전달하기 1234567url = 'https://news.v.daum.net/v/20190728165812603'headers = { 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}resp = requests.get(url, headers=headers)resp.text HTTP response 처리하기 response 객체의 이해 status_code 확인하기 text 속성 확인하기 123456url = 'https://news.v.daum.net/v/20190728165812603'resp = requests.get(url)if resp.status_code == 200: resp.headerselse: print('error') BeautifulSoup의 find와 find_all 함수 find 함수 조건에 만족하는 하나의 tag만 검색 특정 html tag를 검색 검색 조건을 명시하여 찾고자하는 tag를 검색 12345678tag = soup.find('h3')tag.get_text()tag = soup.find('p')tag.get_text()tag = soup.find('div', id='upper')tag.get_text().strip() find_all함수 조건에 맞는 모든 tag를 리스트로 반환합니다. get_text 함수 tag안의 value를 추출 부모tag의 경우, 모든 자식 tag의 value를 추출 attribute 값 추출하기 경우에 따라 추출하고자 하는 값이 attribute에도 존재함 이 경우에는 검색한 tag에 attribute 이름을 [ ]연산을 통해 추출가능 예) div.find(‘h3’)[‘title’] 12tag = soup.find('h3')tag['title'] CSS의 select_one과 select 함수 CSS를 이용하여 tag 찾기 select, select_one함수 사용 css selector 사용법 태그명 찾기 tag 자손 태그 찾기 - 자손 관계 (tag tag) 자식 태그 찾기 - 다이렉트 자식 관계 (tag &gt; tag) 아이디 찾기 #id 클래스 찾기 .class 속성값 찾기 [name=’test’] 속성값 prefix 찾기 [name ^=’test’] 속성값 suffix 찾기 [name $=’test’] 속성값 substring 찾기 [name *=’test] n번째 자식 tag 찾기 :nth-child(n)","link":"/2020/04/21/webcrawling-3/"},{"title":"webcrawling","text":"web crawling 하기전에 알아둬야 할 사항 예를들어, 네이버 홈페이지를 크롤링한다고 하면 www.naver.com/robots.txt을 브라우저 주소창에 입력하면 로봇 배제 규약에 관한 내용이 나옵니다. robots.txt 내용 요약 모든 로봇 접근 허락User-agent: *Allow : / 모든 로봇 접근 차단User-agent: *Disallow: / 모든 로봇에 디렉토리 3곳 접근 차단User-agent: *Disallow: /cgi-bin/Disallow: /tmp/Disallow: /junk/ 모든 로봇에 특정 파일 접근 차단User-agent: *Disallow: /directory/file.html BadBot 로봇에 모든 파일 접근 차단User-agent: BadBotDisallow: / BadBot과 Googlebot에 특정 디렉토리 접근 차단User-agent: BadBotUser-agent: GooglebotDisallow: /private/ 참고사항 2020년 4월 21일 현재 네이버 로봇 규약 설정출처: https://searchadvisor.naver.com/guide/seo-basic-robots 사이트의 루트 페이지만 수집 허용으로 설정합니다.User-agent: *Disallow: /Allow: /$ sitemap.xml 지정User-agent: *Allow: /Sitemap: http://www.example.com/sitemap.xml 다음 로봇 규약 설정 모든 로봇의 접근 차단User-agent: *Disallow: / 카카오 로봇 규약 설정 모든 로봇의 접근 차단 See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file To ban all spiders from the entire site uncomment the next two lines:User-agent: *Disallow: / 문제가 있거나 오타가 있으면 댓글이나 메일로 알려주세요.감사합니다 :) 자세한 내용은 아래 사이트를 참조하세요.출처: https://gbsb.tistory.com/80출처: https://medium.com/@euncho/robots-txt-e08328c4f0fd출처: https://support.google.com/webmasters/answer/6062596?hl=ko출처: https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%B4%87_%EB%B0%B0%EC%A0%9C_%ED%91%9C%EC%A4%80","link":"/2020/04/21/webcrawling-robots.txt/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"create","slug":"create","link":"/tags/create/"},{"name":"favicon","slug":"favicon","link":"/tags/favicon/"},{"name":"concept","slug":"concept","link":"/tags/concept/"},{"name":"agile","slug":"agile","link":"/tags/agile/"},{"name":"Agile","slug":"Agile","link":"/tags/Agile/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"visit","slug":"visit","link":"/tags/visit/"},{"name":"profile","slug":"profile","link":"/tags/profile/"},{"name":"logo","slug":"logo","link":"/tags/logo/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"project","slug":"project","link":"/tags/project/"},{"name":"eda","slug":"eda","link":"/tags/eda/"},{"name":"LinearRegression","slug":"LinearRegression","link":"/tags/LinearRegression/"},{"name":"html","slug":"html","link":"/tags/html/"},{"name":"test","slug":"test","link":"/tags/test/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"webcrawling","slug":"webcrawling","link":"/tags/webcrawling/"},{"name":"get","slug":"get","link":"/tags/get/"},{"name":"post","slug":"post","link":"/tags/post/"},{"name":"randering","slug":"randering","link":"/tags/randering/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"re","slug":"re","link":"/tags/re/"},{"name":"robots.txt","slug":"robots-txt","link":"/tags/robots-txt/"}],"categories":[{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"favicon","slug":"favicon","link":"/categories/favicon/"},{"name":"concept","slug":"concept","link":"/categories/concept/"},{"name":"create","slug":"hexo/create","link":"/categories/hexo/create/"},{"name":"profile","slug":"hexo/profile","link":"/categories/hexo/profile/"},{"name":"markdown","slug":"markdown","link":"/categories/markdown/"},{"name":"project","slug":"project","link":"/categories/project/"},{"name":"agile","slug":"concept/agile","link":"/categories/concept/agile/"},{"name":"crawling","slug":"crawling","link":"/categories/crawling/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"EDA","slug":"project/EDA","link":"/categories/project/EDA/"},{"name":"LinearRegression","slug":"project/LinearRegression","link":"/categories/project/LinearRegression/"},{"name":"concept","slug":"crawling/concept","link":"/categories/crawling/concept/"},{"name":"re","slug":"python/re","link":"/categories/python/re/"},{"name":"python","slug":"crawling/python","link":"/categories/crawling/python/"}]}